{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "\n",
    "# Read the dataset\n",
    "file_path = '../dataset/sample_dataset.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "columns_to_keep = ['abstract']\n",
    "data = data.filter(columns_to_keep).dropna()\n",
    "\n",
    "# Sample data and split into train and test sets\n",
    "random_sample = data.sample(n=10000, random_state=42)\n",
    "train_data, test_data = train_test_split(random_sample, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 abstract\n",
      "634711  In recent years, the number of applications ut...\n",
      "745444  Quality-of-Service attributes such as performa...\n",
      "832989  In ambulatory electroencephalogram (EEG) healt...\n",
      "598136  Recent studies have shown that the IEEE 802.15...\n",
      "231785  A linear extension of a poset $P$ is a permuta...\n",
      "...                                                   ...\n",
      "253515  On-line signature verification still remains a...\n",
      "51030   The problem of providing throughput fairness i...\n",
      "56468   The Direct Access File System (DAFS) is a dist...\n",
      "623650  Fibrous dysplasia (FD) is a developmental anom...\n",
      "17567   Despite the prevalence of long noncoding RNA (...\n",
      "\n",
      "[8000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Komal\n",
      "[nltk_data]     Agarwal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 abstract\n",
      "634711  recent years, number applications utilizing mo...\n",
      "745444  Quality-of-Service attributes performance reli...\n",
      "832989  ambulatory electroencephalogram (EEG) health c...\n",
      "598136  Recent studies shown IEEE 802.15.4 MAC protoco...\n",
      "231785  linear extension poset $P$ permutation element...\n",
      "...                                                   ...\n",
      "253515  On-line signature verification still remains c...\n",
      "51030   problem providing throughput fairness wired-cu...\n",
      "56468   Direct Access File System (DAFS) distributed f...\n",
      "623650  Fibrous dysplasia (FD) developmental anomaly n...\n",
      "17567   Despite prevalence long noncoding RNA (lncRNA)...\n",
      "\n",
      "[8000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply stopword removal to train_data\n",
    "train_data['abstract'] = train_data['abstract'].apply(remove_stopwords)\n",
    "\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text.lower()\n",
    "\n",
    "# Apply text cleaning to train_data\n",
    "train_data['abstract'] = train_data['abstract'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 abstract\n",
      "634711  recent years number applications utilizing mob...\n",
      "745444  qualityofservice attributes performance reliab...\n",
      "832989  ambulatory electroencephalogram eeg health car...\n",
      "598136  recent studies shown ieee 802154 mac protocol ...\n",
      "231785  linear extension poset p permutation elements ...\n",
      "...                                                   ...\n",
      "253515  online signature verification still remains ch...\n",
      "51030   problem providing throughput fairness wiredcum...\n",
      "56468   direct access file system dafs distributed fil...\n",
      "623650  fibrous dysplasia fd developmental anomaly nor...\n",
      "17567   despite prevalence long noncoding rna lncrna g...\n",
      "\n",
      "[8000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec_df = pd.read_csv('../dataset/node2vec_embeddings.csv')\n",
    "def generate_ngrams(text):\n",
    "    words = text.split()\n",
    "    bigrams = [' '.join(bg) for bg in ngrams(words, 2)]\n",
    "    trigrams = [' '.join(tg) for tg in ngrams(words, 3)]\n",
    "    return words + bigrams + trigrams\n",
    "\n",
    "# Function to find matching n-grams\n",
    "def find_matching_ngrams(words):\n",
    "    matching_words = [word for word in words if word in node2vec_df['node_id'].values]\n",
    "    return matching_words\n",
    "\n",
    "# Apply n-grams generation and matching to train_data\n",
    "train_data['ngrams'] = train_data['abstract'].apply(generate_ngrams)\n",
    "train_data['matched_ngrams'] = train_data['ngrams'].apply(find_matching_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create result dataframe\n",
    "result_df = pd.DataFrame({\n",
    "    'Abstract': train_data['abstract'],\n",
    "    'Matched_Ngrams': train_data['matched_ngrams']\n",
    "})\n",
    "\n",
    "# Save result to CSV\n",
    "result_df.to_csv('abstracts_with_matched_ngrams.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
